{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ae45209",
   "metadata": {},
   "source": [
    "# Introduction to RDD\n",
    "\n",
    "In this first session, we will introduce what is a [Resilient Distributed Dataset (RDD)](https://databricks.com/glossary/what-is-rdd). Please refer to the original paper for in-depth details\n",
    "{cite:p}`zaharia2012resilient`.\n",
    "\n",
    "To understand how Spark works, we need to understand the essence of RDD. Long story short, an RDD **represents a fault-tolerant collection of elements partitioned across the nodes of a cluster that can be operated in parallel**. We will chop the last sentence into pieces now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9db0e41",
   "metadata": {},
   "source": [
    "## Processing Logic Expressed as RDD Operations\n",
    "\n",
    "We use RDDs to express a certain processic logic we want to apply to a dataset. For example, finding the average number of days required to recover from a certain disease.\n",
    "\n",
    "Then, Spark makes its magic to **schedule** and **execute** our data processing logic on a distributed fashion.\n",
    "\n",
    "If we look behind the scenes, the Spark runtime requires to determine the order of execution of RDDs, and make sure the execution is fault-tolerant. It uses the following pieces of information for the aforementioned purposes: \n",
    "\n",
    "- Lineage\n",
    "  - Dependencies on parent RDDs\n",
    "- Fault-tolerance\n",
    "  - The partitions that makes the whole dataset: used to execute in **parallel** to speed up the computation with **executors**.\n",
    "  - The function for computing all the rows in the dataset: provided by users (i.e., \"you\"). Each **executor** in the cluster execute this function against each row in each partition.\n",
    "\n",
    "With the above information, Spark can reproduce the RDD in case of failure scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498c62ce",
   "metadata": {},
   "source": [
    "## Quick Demo\n",
    "\n",
    "### Prepare a Jupyter Kernel\n",
    "\n",
    "You will need to prepare a Jupyter kernel in order to run this notebook on your local environment.\n",
    "\n",
    "```{note} This course relies on [Poetry](https://python-poetry.org) as package manager. Commands below can might\n",
    "slightly differ if you are using a different strategy (e.g., [Pipenv](https://pipenv.pypa.io/en/latest/) or [Conda](https://docs.conda.io/en/latest/), just to mention a few).\n",
    "```\n",
    "\n",
    "```\n",
    "poetry shell\n",
    "ipython kernel install --name \"bda-labs\" --user\n",
    "```\n",
    "\n",
    "The output from above command is a JSON file located under your Jupyter installation. For example: `/path_to_jupyter_installtion/kernels/bda-labs/kernel.json`\n",
    "\n",
    "It's content will be similar to the following:\n",
    "\n",
    "```json\n",
    "{\n",
    " \"argv\": [\n",
    "  \"<some_path_here>/bda-course/.venv/bin/python\",\n",
    "  \"-m\",\n",
    "  \"ipykernel_launcher\",\n",
    "  \"-f\",\n",
    "  \"{connection_file}\"\n",
    " ],\n",
    " \"display_name\": \"bda-labs\",\n",
    " \"language\": \"python\",\n",
    " \"metadata\": {\n",
    "  \"debugger\": true\n",
    " }\n",
    "}\n",
    "```\n",
    "\n",
    "### Initialize Spark\n",
    "\n",
    "We need to do some preparation first. In the next snippet, we will import the [SparkContext](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.html#pyspark.SparkContext) and [SparkConf](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkConf.html#pyspark.SparkConf) classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3b58159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4055babc",
   "metadata": {},
   "source": [
    "Now we we proceed to create the **configuration** for our application and a **context** which tells Spark how to access the execution environment (local or a cluster). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5995f25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use \"local\" to indicate we're running in local mode\n",
    "conf = SparkConf().setAppName(\"into-to-rdd\").setMaster(\"local\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f48c81",
   "metadata": {},
   "source": [
    "### Define an RDD\n",
    "\n",
    "Let's start by defining a very simple RDD. We can think of it as data points indicating the recovery days for a certain disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c12d1685",
   "metadata": {},
   "outputs": [],
   "source": [
    "recovery_days_per_disease = [(\"disease_1\", [3, 6, 9, 10]), (\"disease_2\", [11, 11, 10, 9])]\n",
    "rdd = sc.parallelize(recovery_days_per_disease)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a577adc",
   "metadata": {},
   "source": [
    "Now we can operate on it. For example, we can compute the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35d628db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('disease_1', 7.0), ('disease_2', 10.25)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mapValues(lambda x: sum(x) / len(x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfa62fc",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "```{bibliography}\n",
    ":style: unsrt\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f824b5ea",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bda-labs",
   "language": "python",
   "name": "bda-labs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
