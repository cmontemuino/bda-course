{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e270646a",
   "metadata": {},
   "source": [
    "# Introduction to the DataFrame API\n",
    "\n",
    "In this section, we will introduce the [DataFrame and Dataset APIs](https://spark.apache.org/docs/latest/sql-programming-guide.html).\n",
    "\n",
    "We will use a small subset from the [Record Linkage Comparison Data Set](https://archive.ics.uci.edu/ml/datasets/record+linkage+comparison+patterns), borrowed from UC Irvine Machine Learning Repository. It consists of several CSV files with match scores for patients in a Germany hospital, but we will use only one of them for the sake of simplicity. Please consult {cite:p}`schmidtmann2009evaluation` and {cite:p}`sariyar2011controlling` for more details regarding the data sets and research. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a0a219",
   "metadata": {},
   "source": [
    "## Setup\n",
    "- Setup a `SparkSession` to work with the Dataset and DataFrame API\n",
    "- Unzip the `scores.zip` file located under `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46b8804c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/hostedtoolcache/Python/3.9.10/x64/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/07 17:03:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"intro-to-df\").setMaster(\"local\")\n",
    "sc = SparkContext(conf=conf)\n",
    "# Avoid polluting the console with warning messages\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdba91de",
   "metadata": {},
   "source": [
    "### Create a SparkSession to work with the DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55551972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76dab38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SparkSession in module pyspark.sql.session:\n",
      "\n",
      "class SparkSession(pyspark.sql.pandas.conversion.SparkConversionMixin)\n",
      " |  SparkSession(sparkContext, jsparkSession=None)\n",
      " |  \n",
      " |  The entry point to programming Spark with the Dataset and DataFrame API.\n",
      " |  \n",
      " |  A SparkSession can be used create :class:`DataFrame`, register :class:`DataFrame` as\n",
      " |  tables, execute SQL over tables, cache tables, and read parquet files.\n",
      " |  To create a :class:`SparkSession`, use the following builder pattern:\n",
      " |  \n",
      " |  .. autoattribute:: builder\n",
      " |     :annotation:\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> spark = SparkSession.builder \\\n",
      " |  ...     .master(\"local\") \\\n",
      " |  ...     .appName(\"Word Count\") \\\n",
      " |  ...     .config(\"spark.some.config.option\", \"some-value\") \\\n",
      " |  ...     .getOrCreate()\n",
      " |  \n",
      " |  >>> from datetime import datetime\n",
      " |  >>> from pyspark.sql import Row\n",
      " |  >>> spark = SparkSession(sc)\n",
      " |  >>> allTypes = sc.parallelize([Row(i=1, s=\"string\", d=1.0, l=1,\n",
      " |  ...     b=True, list=[1, 2, 3], dict={\"s\": 0}, row=Row(a=1),\n",
      " |  ...     time=datetime(2014, 8, 1, 14, 1, 5))])\n",
      " |  >>> df = allTypes.toDF()\n",
      " |  >>> df.createOrReplaceTempView(\"allTypes\")\n",
      " |  >>> spark.sql('select i+1, d+1, not b, list[1], dict[\"s\"], time, row.a '\n",
      " |  ...            'from allTypes where b and i > 0').collect()\n",
      " |  [Row((i + 1)=2, (d + 1)=2.0, (NOT b)=False, list[1]=2,         dict[s]=0, time=datetime.datetime(2014, 8, 1, 14, 1, 5), a=1)]\n",
      " |  >>> df.rdd.map(lambda x: (x.i, x.s, x.d, x.l, x.b, x.time, x.row.a, x.list)).collect()\n",
      " |  [(1, 'string', 1.0, 1, True, datetime.datetime(2014, 8, 1, 14, 1, 5), 1, [1, 2, 3])]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SparkSession\n",
      " |      pyspark.sql.pandas.conversion.SparkConversionMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      Enable 'with SparkSession.builder.(...).getOrCreate() as session: app' syntax.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  __exit__(self, exc_type, exc_val, exc_tb)\n",
      " |      Enable 'with SparkSession.builder.(...).getOrCreate() as session: app' syntax.\n",
      " |      \n",
      " |      Specifically stop the SparkSession on exit of the with block.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  __init__(self, sparkContext, jsparkSession=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  createDataFrame(self, data, schema=None, samplingRatio=None, verifySchema=True)\n",
      " |      Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n",
      " |      \n",
      " |      When ``schema`` is a list of column names, the type of each column\n",
      " |      will be inferred from ``data``.\n",
      " |      \n",
      " |      When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      " |      from ``data``, which should be an RDD of either :class:`Row`,\n",
      " |      :class:`namedtuple`, or :class:`dict`.\n",
      " |      \n",
      " |      When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must match\n",
      " |      the real data, or an exception will be thrown at runtime. If the given schema is not\n",
      " |      :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      " |      :class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\".\n",
      " |      Each record will also be wrapped into a tuple, which can be converted to row later.\n",
      " |      \n",
      " |      If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\n",
      " |      rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      .. versionchanged:: 2.1.0\n",
      " |         Added verifySchema.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : :class:`RDD` or iterable\n",
      " |          an RDD of any kind of SQL data representation (:class:`Row`,\n",
      " |          :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`, or\n",
      " |          :class:`pandas.DataFrame`.\n",
      " |      schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
      " |          a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      " |          column names, default is None.  The data type string format equals to\n",
      " |          :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      " |          omit the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use\n",
      " |          ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`.\n",
      " |          We can also use ``int`` as a short name for :class:`pyspark.sql.types.IntegerType`.\n",
      " |      samplingRatio : float, optional\n",
      " |          the sample ratio of rows used for inferring\n",
      " |      verifySchema : bool, optional\n",
      " |          verify data types of every row against schema. Enabled by default.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> l = [('Alice', 1)]\n",
      " |      >>> spark.createDataFrame(l).collect()\n",
      " |      [Row(_1='Alice', _2=1)]\n",
      " |      >>> spark.createDataFrame(l, ['name', 'age']).collect()\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      \n",
      " |      >>> d = [{'name': 'Alice', 'age': 1}]\n",
      " |      >>> spark.createDataFrame(d).collect()\n",
      " |      [Row(age=1, name='Alice')]\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(l)\n",
      " |      >>> spark.createDataFrame(rdd).collect()\n",
      " |      [Row(_1='Alice', _2=1)]\n",
      " |      >>> df = spark.createDataFrame(rdd, ['name', 'age'])\n",
      " |      >>> df.collect()\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> Person = Row('name', 'age')\n",
      " |      >>> person = rdd.map(lambda r: Person(*r))\n",
      " |      >>> df2 = spark.createDataFrame(person)\n",
      " |      >>> df2.collect()\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      \n",
      " |      >>> from pyspark.sql.types import *\n",
      " |      >>> schema = StructType([\n",
      " |      ...    StructField(\"name\", StringType(), True),\n",
      " |      ...    StructField(\"age\", IntegerType(), True)])\n",
      " |      >>> df3 = spark.createDataFrame(rdd, schema)\n",
      " |      >>> df3.collect()\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      \n",
      " |      >>> spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      " |      [Row(0=1, 1=2)]\n",
      " |      \n",
      " |      >>> spark.createDataFrame(rdd, \"a: string, b: int\").collect()\n",
      " |      [Row(a='Alice', b=1)]\n",
      " |      >>> rdd = rdd.map(lambda row: row[1])\n",
      " |      >>> spark.createDataFrame(rdd, \"int\").collect()\n",
      " |      [Row(value=1)]\n",
      " |      >>> spark.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      Py4JJavaError: ...\n",
      " |  \n",
      " |  newSession(self)\n",
      " |      Returns a new :class:`SparkSession` as new session, that has separate SQLConf,\n",
      " |      registered temporary views and UDFs, but shared :class:`SparkContext` and\n",
      " |      table cache.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  range(self, start, end=None, step=1, numPartitions=None)\n",
      " |      Create a :class:`DataFrame` with single :class:`pyspark.sql.types.LongType` column named\n",
      " |      ``id``, containing elements in a range from ``start`` to ``end`` (exclusive) with\n",
      " |      step value ``step``.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      start : int\n",
      " |          the start value\n",
      " |      end : int, optional\n",
      " |          the end value (exclusive)\n",
      " |      step : int, optional\n",
      " |          the incremental step (default: 1)\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions of the DataFrame\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.range(1, 7, 2).collect()\n",
      " |      [Row(id=1), Row(id=3), Row(id=5)]\n",
      " |      \n",
      " |      If only one argument is specified, it will be used as the end value.\n",
      " |      \n",
      " |      >>> spark.range(3).collect()\n",
      " |      [Row(id=0), Row(id=1), Row(id=2)]\n",
      " |  \n",
      " |  sql(self, sqlQuery)\n",
      " |      Returns a :class:`DataFrame` representing the result of the given query.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.createOrReplaceTempView(\"table1\")\n",
      " |      >>> df2 = spark.sql(\"SELECT field1 AS f1, field2 as f2 from table1\")\n",
      " |      >>> df2.collect()\n",
      " |      [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\n",
      " |  \n",
      " |  stop(self)\n",
      " |      Stop the underlying :class:`SparkContext`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  table(self, tableName)\n",
      " |      Returns the specified table as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.createOrReplaceTempView(\"table1\")\n",
      " |      >>> df2 = spark.table(\"table1\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  getActiveSession() from builtins.type\n",
      " |      Returns the active :class:`SparkSession` for the current thread, returned by the builder\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`SparkSession`\n",
      " |          Spark session if an active session exists for the current thread\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> s = SparkSession.getActiveSession()\n",
      " |      >>> l = [('Alice', 1)]\n",
      " |      >>> rdd = s.sparkContext.parallelize(l)\n",
      " |      >>> df = s.createDataFrame(rdd, ['name', 'age'])\n",
      " |      >>> df.select(\"age\").collect()\n",
      " |      [Row(age=1)]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  catalog\n",
      " |      Interface through which the user may create, drop, alter or query underlying\n",
      " |      databases, tables, functions, etc.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`Catalog`\n",
      " |  \n",
      " |  conf\n",
      " |      Runtime configuration interface for Spark.\n",
      " |      \n",
      " |      This is the interface through which the user can get and set all Spark and Hadoop\n",
      " |      configurations that are relevant to Spark SQL. When getting the value of a config,\n",
      " |      this defaults to the value set in the underlying :class:`SparkContext`, if any.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`pyspark.sql.conf.RuntimeConfig`\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  read\n",
      " |      Returns a :class:`DataFrameReader` that can be used to read data\n",
      " |      in as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrameReader`\n",
      " |  \n",
      " |  readStream\n",
      " |      Returns a :class:`DataStreamReader` that can be used to read data streams\n",
      " |      as a streaming :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is evolving.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataStreamReader`\n",
      " |  \n",
      " |  sparkContext\n",
      " |      Returns the underlying :class:`SparkContext`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  streams\n",
      " |      Returns a :class:`StreamingQueryManager` that allows managing all the\n",
      " |      :class:`StreamingQuery` instances active on `this` context.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is evolving.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`StreamingQueryManager`\n",
      " |  \n",
      " |  udf\n",
      " |      Returns a :class:`UDFRegistration` for UDF registration.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`UDFRegistration`\n",
      " |  \n",
      " |  version\n",
      " |      The version of Spark on which this application is running.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  Builder = <class 'pyspark.sql.session.SparkSession.Builder'>\n",
      " |      Builder for :class:`SparkSession`.\n",
      " |  \n",
      " |  \n",
      " |  builder = <pyspark.sql.session.SparkSession.Builder object>\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.sql.pandas.conversion.SparkConversionMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(SparkSession)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e10026",
   "metadata": {},
   "source": [
    "### Unzip the scores file, if it was not done already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d936fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SCORES_ZIP=data/scores.zip\n",
      "env: SCORES_CSV=data/scores.csv\n"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "scores_zip = path.join(\"data\", \"scores.zip\")\n",
    "scores_csv = path.join(\"data\", \"scores.csv\")\n",
    "\n",
    "%set_env SCORES_ZIP=$scores_zip\n",
    "%set_env SCORES_CSV=$scores_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a735dcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzip file data/scores.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data/scores.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  inflating: data/scores.csv         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  inflating: data/__MACOSX/._scores.csv  \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "command -v unzip >/dev/null 2>&1 || { echo >&2 \"unzip command is not installed. Aborting.\"; exit 1; }\n",
    "[[ -f \"$SCORES_CSV\" ]] && { echo \"file data/$SCORES_CSV already exist. Skipping.\"; exit 0; }\n",
    "\n",
    "[[ -f \"$SCORES_ZIP\" ]] || { echo \"file data/$SCORES_ZIP does not exist. Aborting.\"; exit 1; }\n",
    "\n",
    "echo \"Unzip file $SCORES_ZIP\"\n",
    "unzip \"$SCORES_ZIP\" -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42c2515a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"id_1\",\"id_2\",\"cmp_fname_c1\",\"cmp_fname_c2\",\"cmp_lname_c1\",\"cmp_lname_c2\",\"cmp_sex\",\"cmp_bd\",\"cmp_bm\",\"cmp_by\",\"cmp_plz\",\"is_match\"\r\n",
      "37291,53113,0.833333333333333,?,1,?,1,1,1,1,0,TRUE\r\n",
      "39086,47614,1,?,1,?,1,1,1,1,1,TRUE\r\n",
      "70031,70237,1,?,1,?,1,1,1,1,1,TRUE\r\n",
      "84795,97439,1,?,1,?,1,1,1,1,1,TRUE\r\n",
      "36950,42116,1,?,1,1,1,1,1,1,1,TRUE\r\n",
      "42413,48491,1,?,1,?,1,1,1,1,1,TRUE\r\n",
      "25965,64753,1,?,1,?,1,1,1,1,1,TRUE\r\n",
      "49451,90407,1,?,1,?,1,1,1,1,0,TRUE\r\n",
      "39932,40902,1,?,1,?,1,1,1,1,1,TRUE\r\n"
     ]
    }
   ],
   "source": [
    "! head \"$SCORES_CSV\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25100f59",
   "metadata": {},
   "source": [
    "## Loading the Scores CSV file into a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b08f5d",
   "metadata": {},
   "source": [
    "We are going to use the Reader API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7bf8470",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DataFrameReader in module pyspark.sql.readwriter object:\n",
      "\n",
      "class DataFrameReader(OptionUtils)\n",
      " |  DataFrameReader(spark)\n",
      " |  \n",
      " |  Interface used to load a :class:`DataFrame` from external storage systems\n",
      " |  (e.g. file systems, key-value stores, etc). Use :attr:`SparkSession.read`\n",
      " |  to access this.\n",
      " |  \n",
      " |  .. versionadded:: 1.4\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DataFrameReader\n",
      " |      OptionUtils\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, spark)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  csv(self, path, schema=None, sep=None, encoding=None, quote=None, escape=None, comment=None, header=None, inferSchema=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, nullValue=None, nanValue=None, positiveInf=None, negativeInf=None, dateFormat=None, timestampFormat=None, maxColumns=None, maxCharsPerColumn=None, maxMalformedLogPerPartition=None, mode=None, columnNameOfCorruptRecord=None, multiLine=None, charToEscapeQuoteEscaping=None, samplingRatio=None, enforceSchema=None, emptyValue=None, locale=None, lineSep=None, pathGlobFilter=None, recursiveFileLookup=None, modifiedBefore=None, modifiedAfter=None, unescapedQuoteHandling=None)\n",
      " |      Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
      " |      \n",
      " |      This function will go through the input once to determine the input schema if\n",
      " |      ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
      " |      ``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str or list\n",
      " |          string, or list of strings, for input path(s),\n",
      " |          or RDD of Strings storing CSV rows.\n",
      " |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
      " |          an optional :class:`pyspark.sql.types.StructType` for the input schema\n",
      " |          or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      Extra options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
      " |          in the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.read.csv('python/test_support/sql/ages.csv')\n",
      " |      >>> df.dtypes\n",
      " |      [('_c0', 'string'), ('_c1', 'string')]\n",
      " |      >>> rdd = sc.textFile('python/test_support/sql/ages.csv')\n",
      " |      >>> df2 = spark.read.csv(rdd)\n",
      " |      >>> df2.dtypes\n",
      " |      [('_c0', 'string'), ('_c1', 'string')]\n",
      " |  \n",
      " |  format(self, source)\n",
      " |      Specifies the input data source format.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      source : str\n",
      " |          string, name of the data source, e.g. 'json', 'parquet'.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.read.format('json').load('python/test_support/sql/people.json')\n",
      " |      >>> df.dtypes\n",
      " |      [('age', 'bigint'), ('name', 'string')]\n",
      " |  \n",
      " |  jdbc(self, url, table, column=None, lowerBound=None, upperBound=None, numPartitions=None, predicates=None, properties=None)\n",
      " |      Construct a :class:`DataFrame` representing the database table named ``table``\n",
      " |      accessible via JDBC URL ``url`` and connection ``properties``.\n",
      " |      \n",
      " |      Partitions of the table will be retrieved in parallel if either ``column`` or\n",
      " |      ``predicates`` is specified. ``lowerBound``, ``upperBound`` and ``numPartitions``\n",
      " |      is needed when ``column`` is specified.\n",
      " |      \n",
      " |      If both ``column`` and ``predicates`` are specified, ``column`` will be used.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      table : str\n",
      " |          the name of the table\n",
      " |      column : str, optional\n",
      " |          alias of ``partitionColumn`` option. Refer to ``partitionColumn`` in\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n",
      " |          in the version you use.\n",
      " |      predicates : list, optional\n",
      " |          a list of expressions suitable for inclusion in WHERE clauses;\n",
      " |          each one defines one partition of the :class:`DataFrame`\n",
      " |      properties : dict, optional\n",
      " |          a dictionary of JDBC database connection arguments. Normally at\n",
      " |          least properties \"user\" and \"password\" with their corresponding values.\n",
      " |          For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      Extra options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n",
      " |          in the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Don't create too many partitions in parallel on a large cluster;\n",
      " |      otherwise Spark might crash your external database systems.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |  \n",
      " |  json(self, path, schema=None, primitivesAsString=None, prefersDecimal=None, allowComments=None, allowUnquotedFieldNames=None, allowSingleQuotes=None, allowNumericLeadingZero=None, allowBackslashEscapingAnyCharacter=None, mode=None, columnNameOfCorruptRecord=None, dateFormat=None, timestampFormat=None, multiLine=None, allowUnquotedControlChars=None, lineSep=None, samplingRatio=None, dropFieldIfAllNull=None, encoding=None, locale=None, pathGlobFilter=None, recursiveFileLookup=None, allowNonNumericNumbers=None, modifiedBefore=None, modifiedAfter=None)\n",
      " |      Loads JSON files and returns the results as a :class:`DataFrame`.\n",
      " |      \n",
      " |      `JSON Lines <http://jsonlines.org/>`_ (newline-delimited JSON) is supported by default.\n",
      " |      For JSON (one record per file), set the ``multiLine`` parameter to ``true``.\n",
      " |      \n",
      " |      If the ``schema`` parameter is not specified, this function goes\n",
      " |      through the input once to determine the input schema.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str, list or :class:`RDD`\n",
      " |          string represents path to the JSON dataset, or a list of paths,\n",
      " |          or RDD of Strings storing JSON objects.\n",
      " |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
      " |          an optional :class:`pyspark.sql.types.StructType` for the input schema or\n",
      " |          a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      Extra options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n",
      " |          in the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df1 = spark.read.json('python/test_support/sql/people.json')\n",
      " |      >>> df1.dtypes\n",
      " |      [('age', 'bigint'), ('name', 'string')]\n",
      " |      >>> rdd = sc.textFile('python/test_support/sql/people.json')\n",
      " |      >>> df2 = spark.read.json(rdd)\n",
      " |      >>> df2.dtypes\n",
      " |      [('age', 'bigint'), ('name', 'string')]\n",
      " |  \n",
      " |  load(self, path=None, format=None, schema=None, **options)\n",
      " |      Loads data from a data source and returns it as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str or list, optional\n",
      " |          optional string or a list of string for file-system backed data sources.\n",
      " |      format : str, optional\n",
      " |          optional string for format of the data source. Default to 'parquet'.\n",
      " |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
      " |          optional :class:`pyspark.sql.types.StructType` for the input schema\n",
      " |          or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      " |      **options : dict\n",
      " |          all other string options\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.read.format(\"parquet\").load('python/test_support/sql/parquet_partitioned',\n",
      " |      ...     opt1=True, opt2=1, opt3='str')\n",
      " |      >>> df.dtypes\n",
      " |      [('name', 'string'), ('year', 'int'), ('month', 'int'), ('day', 'int')]\n",
      " |      \n",
      " |      >>> df = spark.read.format('json').load(['python/test_support/sql/people.json',\n",
      " |      ...     'python/test_support/sql/people1.json'])\n",
      " |      >>> df.dtypes\n",
      " |      [('age', 'bigint'), ('aka', 'string'), ('name', 'string')]\n",
      " |  \n",
      " |  option(self, key, value)\n",
      " |      Adds an input option for the underlying data source.\n",
      " |      \n",
      " |      .. versionadded:: 1.5\n",
      " |  \n",
      " |  options(self, **options)\n",
      " |      Adds input options for the underlying data source.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  orc(self, path, mergeSchema=None, pathGlobFilter=None, recursiveFileLookup=None, modifiedBefore=None, modifiedAfter=None)\n",
      " |      Loads ORC files, returning the result as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str or list\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      Extra options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-orc.html#data-source-option>`_\n",
      " |          in the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.read.orc('python/test_support/sql/orc_partitioned')\n",
      " |      >>> df.dtypes\n",
      " |      [('a', 'bigint'), ('b', 'int'), ('c', 'int')]\n",
      " |  \n",
      " |  parquet(self, *paths, **options)\n",
      " |      Loads Parquet files, returning the result as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      paths : str\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#data-source-option>`_\n",
      " |          in the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.read.parquet('python/test_support/sql/parquet_partitioned')\n",
      " |      >>> df.dtypes\n",
      " |      [('name', 'string'), ('year', 'int'), ('month', 'int'), ('day', 'int')]\n",
      " |  \n",
      " |  schema(self, schema)\n",
      " |      Specifies the input schema.\n",
      " |      \n",
      " |      Some data sources (e.g. JSON) can infer the input schema automatically from data.\n",
      " |      By specifying the schema here, the underlying data source can skip the schema\n",
      " |      inference step, and thus speed up data loading.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      schema : :class:`pyspark.sql.types.StructType` or str\n",
      " |          a :class:`pyspark.sql.types.StructType` object or a DDL-formatted string\n",
      " |          (For example ``col0 INT, col1 DOUBLE``).\n",
      " |      \n",
      " |      >>> s = spark.read.schema(\"col0 INT, col1 DOUBLE\")\n",
      " |  \n",
      " |  table(self, tableName)\n",
      " |      Returns the specified table as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      tableName : str\n",
      " |          string, name of the table.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.read.parquet('python/test_support/sql/parquet_partitioned')\n",
      " |      >>> df.createOrReplaceTempView('tmpTable')\n",
      " |      >>> spark.read.table('tmpTable').dtypes\n",
      " |      [('name', 'string'), ('year', 'int'), ('month', 'int'), ('day', 'int')]\n",
      " |  \n",
      " |  text(self, paths, wholetext=False, lineSep=None, pathGlobFilter=None, recursiveFileLookup=None, modifiedBefore=None, modifiedAfter=None)\n",
      " |      Loads text files and returns a :class:`DataFrame` whose schema starts with a\n",
      " |      string column named \"value\", and followed by partitioned columns if there\n",
      " |      are any.\n",
      " |      The text files must be encoded as UTF-8.\n",
      " |      \n",
      " |      By default, each line in the text file is a new row in the resulting DataFrame.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      paths : str or list\n",
      " |          string, or list of strings, for input path(s).\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      Extra options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-text.html#data-source-option>`_\n",
      " |          in the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.read.text('python/test_support/sql/text-test.txt')\n",
      " |      >>> df.collect()\n",
      " |      [Row(value='hello'), Row(value='this')]\n",
      " |      >>> df = spark.read.text('python/test_support/sql/text-test.txt', wholetext=True)\n",
      " |      >>> df.collect()\n",
      " |      [Row(value='hello\\nthis')]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from OptionUtils:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d869a11",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method csv in module pyspark.sql.readwriter:\n",
      "\n",
      "csv(path, schema=None, sep=None, encoding=None, quote=None, escape=None, comment=None, header=None, inferSchema=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, nullValue=None, nanValue=None, positiveInf=None, negativeInf=None, dateFormat=None, timestampFormat=None, maxColumns=None, maxCharsPerColumn=None, maxMalformedLogPerPartition=None, mode=None, columnNameOfCorruptRecord=None, multiLine=None, charToEscapeQuoteEscaping=None, samplingRatio=None, enforceSchema=None, emptyValue=None, locale=None, lineSep=None, pathGlobFilter=None, recursiveFileLookup=None, modifiedBefore=None, modifiedAfter=None, unescapedQuoteHandling=None) method of pyspark.sql.readwriter.DataFrameReader instance\n",
      "    Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
      "    \n",
      "    This function will go through the input once to determine the input schema if\n",
      "    ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
      "    ``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
      "    \n",
      "    .. versionadded:: 2.0.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    path : str or list\n",
      "        string, or list of strings, for input path(s),\n",
      "        or RDD of Strings storing CSV rows.\n",
      "    schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
      "        an optional :class:`pyspark.sql.types.StructType` for the input schema\n",
      "        or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      "    \n",
      "    Other Parameters\n",
      "    ----------------\n",
      "    Extra options\n",
      "        For the extra options, refer to\n",
      "        `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
      "        in the version you use.\n",
      "    \n",
      "        .. # noqa\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.read.csv('python/test_support/sql/ages.csv')\n",
      "    >>> df.dtypes\n",
      "    [('_c0', 'string'), ('_c1', 'string')]\n",
      "    >>> rdd = sc.textFile('python/test_support/sql/ages.csv')\n",
      "    >>> df2 = spark.read.csv(rdd)\n",
      "    >>> df2.dtypes\n",
      "    [('_c0', 'string'), ('_c1', 'string')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.read.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32c74e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = spark.read.csv(scores_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62fa8877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "200b6934",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method show in module pyspark.sql.dataframe:\n",
      "\n",
      "show(n=20, truncate=True, vertical=False) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Prints the first ``n`` rows to the console.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    n : int, optional\n",
      "        Number of rows to show.\n",
      "    truncate : bool or int, optional\n",
      "        If set to ``True``, truncate strings longer than 20 chars by default.\n",
      "        If set to a number greater than one, truncates long strings to length ``truncate``\n",
      "        and align cells right.\n",
      "    vertical : bool, optional\n",
      "        If set to ``True``, print output rows vertically (one line\n",
      "        per column value).\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df\n",
      "    DataFrame[age: int, name: string]\n",
      "    >>> df.show()\n",
      "    +---+-----+\n",
      "    |age| name|\n",
      "    +---+-----+\n",
      "    |  2|Alice|\n",
      "    |  5|  Bob|\n",
      "    +---+-----+\n",
      "    >>> df.show(truncate=3)\n",
      "    +---+----+\n",
      "    |age|name|\n",
      "    +---+----+\n",
      "    |  2| Ali|\n",
      "    |  5| Bob|\n",
      "    +---+----+\n",
      "    >>> df.show(vertical=True)\n",
      "    -RECORD 0-----\n",
      "     age  | 2\n",
      "     name | Alice\n",
      "    -RECORD 1-----\n",
      "     age  | 5\n",
      "     name | Bob\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(scores.show)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bf3da7",
   "metadata": {},
   "source": [
    "We can look at the head of the DataFrame calling the `show` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d739a53a",
   "metadata": {},
   "source": [
    "scores.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bd6569",
   "metadata": {},
   "source": [
    "**Can anyone spot what's wrong with the above data?**\n",
    "\n",
    "- Question marks\n",
    "- Column names\n",
    "- `Float` and `Int` in the same column\n",
    "\n",
    "Let's check the schema of our DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd968d53",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method printSchema in module pyspark.sql.dataframe:\n",
      "\n",
      "printSchema() method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Prints out the schema in the tree format.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.printSchema()\n",
      "    root\n",
      "     |-- age: integer (nullable = true)\n",
      "     |-- name: string (nullable = true)\n",
      "    <BLANKLINE>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(scores.printSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "073b53fb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e970a9",
   "metadata": {},
   "source": [
    "**Why everythin is a `String`?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185059b1",
   "metadata": {},
   "source": [
    "### Managing Schema and Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae4c5bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "scores_df = (\n",
    "    spark.read\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"nullValue\", \"?\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .csv(scores_csv)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8c9fb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_1: integer (nullable = true)\n",
      " |-- id_2: integer (nullable = true)\n",
      " |-- cmp_fname_c1: double (nullable = true)\n",
      " |-- cmp_fname_c2: double (nullable = true)\n",
      " |-- cmp_lname_c1: double (nullable = true)\n",
      " |-- cmp_lname_c2: double (nullable = true)\n",
      " |-- cmp_sex: integer (nullable = true)\n",
      " |-- cmp_bd: integer (nullable = true)\n",
      " |-- cmp_bm: integer (nullable = true)\n",
      " |-- cmp_by: integer (nullable = true)\n",
      " |-- cmp_plz: integer (nullable = true)\n",
      " |-- is_match: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dc20714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| id_1| id_2|     cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "|37291|53113|0.833333333333333|        null|         1.0|        null|      1|     1|     1|     1|      0|    true|\n",
      "|39086|47614|              1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|70031|70237|              1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|84795|97439|              1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|36950|42116|              1.0|        null|         1.0|         1.0|      1|     1|     1|     1|      1|    true|\n",
      "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcf65b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd4cf67b",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "```{bibliography}\n",
    ":style: unsrt\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bda-labs",
   "language": "python",
   "name": "bda-labs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}