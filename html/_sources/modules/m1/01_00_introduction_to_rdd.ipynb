{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ae45209",
   "metadata": {},
   "source": [
    "# Introduction to RDD\n",
    "\n",
    "In this first session, we will introduce what is a [Resilient Distributed Dataset (RDD)](https://databricks.com/glossary/what-is-rdd). Please refer to the original paper for in-depth details\n",
    "{cite:p}`zaharia2012resilient`.\n",
    "\n",
    "To understand how Spark works, we need to understand the essence of RDD. Long story short, an RDD **represents a fault-tolerant collection of elements partitioned across the nodes of a cluster that can be operated in parallel**. We will chop the last sentence into pieces now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9db0e41",
   "metadata": {},
   "source": [
    "## Processing Logic Expressed as RDD Operations\n",
    "\n",
    "We use RDDs to express a certain processic logic we want to apply to a dataset. For example, finding the average number of days required to recover from a certain disease.\n",
    "\n",
    "Then, Spark makes its magic to **schedule** and **execute** our data processing logic on a distributed fashion.\n",
    "\n",
    "If we look behind the scenes, the Spark runtime requires to determine the order of execution of RDDs, and make sure the execution is fault-tolerant. It uses the following pieces of information for the aforementioned purposes: \n",
    "\n",
    "- Lineage\n",
    "  - Dependencies on parent RDDs\n",
    "- Fault-tolerance\n",
    "  - The partitions that makes the whole dataset: used to execute in **parallel** to speed up the computation with **executors**.\n",
    "  - The function for computing all the rows in the dataset: provided by users (i.e., \"you\"). Each **executor** in the cluster execute this function against each row in each partition.\n",
    "\n",
    "With the above information, Spark can reproduce the RDD in case of failure scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498c62ce",
   "metadata": {},
   "source": [
    "## Quick Demo\n",
    "\n",
    "### Prepare a Jupyter Kernel\n",
    "\n",
    "You will need to prepare a Jupyter kernel in order to run this notebook on your local environment.\n",
    "\n",
    "```{note}\n",
    "This course relies on [Poetry](https://python-poetry.org) as package manager. Commands below can might\n",
    "slightly differ if you are using a different strategy (e.g., [Pipenv](https://pipenv.pypa.io/en/latest/) or [Conda](https://docs.conda.io/en/latest/), just to mention a few).\n",
    "```\n",
    "\n",
    "```\n",
    "poetry shell\n",
    "ipython kernel install --name \"bda-labs\" --user\n",
    "```\n",
    "\n",
    "<details>\n",
    "The output from above command is a JSON file located under your Jupyter installation. For example: `/path_to_jupyter_installtion/kernels/bda-labs/kernel.json`\n",
    "\n",
    "It's content will be similar to the following:\n",
    "\n",
    "```json\n",
    "{\n",
    " \"argv\": [\n",
    "  \"<some_path_here>/bda-course/.venv/bin/python\",\n",
    "  \"-m\",\n",
    "  \"ipykernel_launcher\",\n",
    "  \"-f\",\n",
    "  \"{connection_file}\"\n",
    " ],\n",
    " \"display_name\": \"bda-labs\",\n",
    " \"language\": \"python\",\n",
    " \"metadata\": {\n",
    "  \"debugger\": true\n",
    " }\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "### Initialize Spark\n",
    "\n",
    "We need to do some preparation first. In the next snippet, we will import the [SparkContext](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.html#pyspark.SparkContext) and [SparkConf](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkConf.html#pyspark.SparkConf) classes.\n",
    "\n",
    "Now we we proceed to create the **configuration** for our application and a **context** which tells Spark how to access the execution environment (local or a cluster). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f994df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# we use \"local\" to indicate we're running in local mode\n",
    "conf = SparkConf().setAppName(\"into-to-rdd\").setMaster(\"local\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f48c81",
   "metadata": {},
   "source": [
    "#### SparkConf\n",
    "    \n",
    "We triggered a couple of actions with the above code. To start with, we created a `SparkConfig` object and chained two important configurations:\n",
    "    - The Spark application name.\n",
    "    - The URL where the `master node` can be located. **Setting it to `\"local\"` is a special configuration to indicate we are running on the same host where this code is being executed.**\n",
    "    \n",
    "**Note**: it is important to notice that Spark runs on a JVM. When a `SparkConf` is created, it will load values from `spark.*` Java system properties. If we make such properties available to the process running Spark, then they would be picked up.\n",
    "\n",
    "Let's inspect a couple of common Spark configuration properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169c33ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"spark.app.name: {conf.get('spark.app.name')}\")\n",
    "print(f\"spark.master: {conf.get('spark.master')}\")\n",
    "print(f\"spark.home: {conf.get('spark.home')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c6203c",
   "metadata": {},
   "source": [
    "So far so good. We get the same value we chained to the `SparkConf` object before.\n",
    "\n",
    "**Homework/self-research**:\n",
    "\n",
    "- Why don't we have a value for `spark.home`?\n",
    "- Should we care?\n",
    "\n",
    "Should we want to know all the properties available, then the following snippet might help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902b88b1",
   "metadata": {},
   "outputs": [],
   "source": [
    " for p in sorted(conf.getAll(), key=lambda p: p[0]):\n",
    "     print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d62ab9",
   "metadata": {},
   "source": [
    "#### SparkContext\n",
    "\n",
    "`SparkContext` is at the hearth of Spark. It is the main entry point for Spark that representes the connection of the \"driver program\" to a Spark cluster. **You must get familiarized with it**.\n",
    "\n",
    "We can pass several parameters to it, from which two of them are mandatory: `master` and `appName`. In our case, we passed such information wrapped into a `SparkConf` object.\n",
    "\n",
    "```{attention}\n",
    "\n",
    "- `SparkContext` is **always** created on the driver and **it is not serialized**. Thus, it cannot be shipped to workers.\n",
    "- We can have one `SparkContext` per application at most. Try to run the cell where we created the `SparkContext` and see what happens!\n",
    "```\n",
    "\n",
    "\n",
    "```{note}\n",
    "Actually, only one `SparkContext` can be active per JVM. If we want to create a new one, we must call the `stop()` method to our existing `SparkContext` object first.\n",
    "```\n",
    "\n",
    "Another consequence of passing a `SparkConf` object to the `SparkContext` constructor is **immutable configuration**. The `SparkConf` object is cloned and can no longer be modified. Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeec1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is our SparkConf object the same we get from SparkContext?\n",
    "print(f\"conf == sc.getConf() --> {conf == sc.getConf()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a43694a",
   "metadata": {},
   "source": [
    "\n",
    "### Define an RDD\n",
    "\n",
    "Let's start by defining a very simple RDD. We can think of it as data points indicating the recovery days for a certain disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12d1685",
   "metadata": {},
   "outputs": [],
   "source": [
    "recovery_days_per_disease = [(\"disease_1\", [3, 6, 9, 10]), (\"disease_2\", [11, 11, 10, 9])]\n",
    "rdd = sc.parallelize(recovery_days_per_disease)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a577adc",
   "metadata": {},
   "source": [
    "Now we can operate on it. For example, we can compute the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d628db",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.mapValues(lambda x: sum(x) / len(x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfa62fc",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "```{bibliography}\n",
    ":style: unsrt\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f824b5ea",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bda-labs",
   "language": "python",
   "name": "bda-labs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
